import numpy as np
from typing import List, Tuple

class LearnableCortex:
    """
    Layer 1 (The Mind) - Digital Cortex.
    
    A trainable Neural Network that acts as the 'Long-Term Memory' (LTM) 
    for the Mem4ristor system.
    
    Mechanism:
    - During the day (Waking), serves as a classifier/predictor.
    - During the night (Sleep), learns from the 'Dream Logs' generated by the 
      Limbic System (Mem4ristor).
    - Uses Auto-Associative Learning (Reconstruction) to 'consolidate' 
      emotional experiences into structural weights.
    """
    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int, seed: int = 42):
        self.rng = np.random.RandomState(seed)
        self.input_dim = input_dim
        
        # Simple MLP: Input -> Hidden -> Output
        # Initialize with small random weights
        self.W1 = self.rng.normal(0, 0.1, (input_dim, hidden_dim))
        self.b1 = np.zeros(hidden_dim)
        
        self.W2 = self.rng.normal(0, 0.1, (hidden_dim, output_dim))
        self.b2 = np.zeros(output_dim)
        
        # Cache for backprop
        self._z1 = None
        self._a1 = None
        self._z2 = None
        
    def forward(self, x: np.ndarray) -> np.ndarray:
        """
        Forward pass.
        
        Args:
            x (np.ndarray): Input vector (shape: input_dim)
            
        Returns:
            np.ndarray: Output vector (shape: output_dim)
        """
        # Layer 1
        self._z1 = x @ self.W1 + self.b1
        self._a1 = np.maximum(0, self._z1) # ReLU
        
        # Layer 2
        self._z2 = self._a1 @ self.W2 + self.b2
        # Linear output (or subsequent Softmax if needed externally)
        return self._z2

    def sleep_and_learn(self, dream_log: np.ndarray, learning_rate: float = 0.01, epochs: int = 1) -> float:
        """
        Consolidate memories from Dream Logs.
        
        The 'dream_log' contains patterns hallucinated by the Mem4ristor.
        The Cortex treats these as 'Ground Truth' for what is important.
        It trains itself to RECONSTRUCT these patterns (Autoencoder logic),
        thereby becoming 'imprinted' with the Mem4ristor's emotional state.
        
        Args:
            dream_log (np.ndarray): Array of shape (steps, input_dim).
            learning_rate (float): Step size.
            epochs (int): Number of passes over the dream.
            
        Returns:
            float: Average reconstruction error (loss).
        """
        if dream_log.shape[1] != self.input_dim:
            raise ValueError(f"Dream dimension {dream_log.shape[1]} does not match Cortex input {self.input_dim}")
            
        total_loss = 0.0
        n_samples = len(dream_log)
        
        for _ in range(epochs):
            epoch_loss = 0.0
            
            for dream_pattern in dream_log:
                # 1. Forward
                output = self.forward(dream_pattern)
                
                # 2. Loss (MSE: We want Output ~= Input)
                # Why Input? Because we want the Cortex to 'internalize' the pattern "as is".
                # Note: This assumes output_dim == input_dim for Autoencoder behavior.
                # If output_dim is different (e.g. Classification), we need labels.
                # In this unsupervised dream context, Reconstruction is the safest assumption for "Consolidation".
                
                error = output - dream_pattern
                loss = np.mean(error**2)
                epoch_loss += loss
                
                # 3. Backprop
                # dL/dOut = 2 * error / N_out (ignoring scalar factor for simplicity)
                d_output = error 
                
                # Layer 2 Gradients
                d_W2 = np.outer(self._a1, d_output)
                d_b2 = d_output
                
                # Layer 1 Gradients
                d_hidden = (d_output @ self.W2.T) * (self._z1 > 0) # ReLU derivative
                d_W1 = np.outer(dream_pattern, d_hidden)
                d_b1 = d_hidden
                
                # 4. Update
                self.W1 -= learning_rate * d_W1
                self.b1 -= learning_rate * d_b1
                self.W2 -= learning_rate * d_W2
                self.b2 -= learning_rate * d_b2
                
            total_loss += epoch_loss
            
        return total_loss / (n_samples * epochs)

    def get_mse_on_pattern(self, pattern: np.ndarray) -> float:
        """Measuse how 'familiar' a pattern is to the Cortex."""
        output = self.forward(pattern)
        return np.mean((output - pattern)**2)
